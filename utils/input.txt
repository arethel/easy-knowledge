Теперь должно быть понятно, почему так важно требование к расположению гиперплоскости регрессии как можно ближе к обучающим образцам: если бы красная
линия на рис. 3.1 была далека от синих точек, прогноз yнов. имел бы меньше шансов
оказаться верным.

3.1.2. Решение
Чтобы удовлетворить это последнее требование, процедура оптимизации, используемая для поиска оптимальных значений w* и b*, должна минимизировать
следующее выражение:

Выражение, которое требуется минимизировать или максимизировать, вматематике называется целевой функцией, или просто целью. Выражение (fw, b (xi
) – yi
)2
в целевой функции выше называется функцией потерь. Она определяет величину штрафа за неправильную классификацию образца i. Эта конкретная
функция потерь называется квадратичной функцией потерь (squared error
loss). Все алгоритмы обучения, основанные на моделях, используют функцию
потерь, и в поисках лучшей модели мы пытаемся минимизировать цель, известную как функция стоимости (cost function). В линейной регрессии функция
стоимости определяется как средняя потеря, также называемая эмпирическим
риском (empirical risk). Средняя потеря, или эмпирический риск, для модели —
это среднее всех штрафов, полученных при применении модели к обучающим
данным.

Почему в линейной регрессии используется квадратичная функция потерь? И можно ли взять абсолютное значение разности между истинным целевым значением yi
и прогнозируемым значением f(xi
) и использовать его в качестве штрафа? Можно.
Более того, можно использовать любую четную степень вместо квадрата.

Теперь, возможно, вы начинаете понимать, сколько решений, на первый взгляд
произвольных, принимается при разработке алгоритма машинного обучения:
в данном примере мы решили использовать линейную комбинацию признаков
для прогнозирования целевого значения. Однако для объединения значений признаков мы могли бы использовать квадрат или другой многочлен. Мы могли бы
также использовать другую функцию потерь, не лишенную смысла: абсолютная
разность между f (xi
) и yi
 имеет смысл, куб разности — тоже; бинарная функция
потерь (возвращающая 1, когда f (xi
) и yi
 различны, и 0 — когда они одинаковы)
тоже имеет смысл, верно?